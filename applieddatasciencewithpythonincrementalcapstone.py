# -*- coding: utf-8 -*-
"""AppliedDataSciencewithPythonIncrementalCapstone.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1qMcEW6-0xNeEWOeaNp_GYN7nR1ArJTEH

# **Applied Data Science with Python - Incremental Capstone**

- Team Member(s):
- - Carllos Watts-Nogueira
- - Ranjan Baral


- Room number: 8

- Jun, 4 2025

# **Overview**
BikeEase is a New York-based urban mobility company providing bike rental services across the city. The company offers flexible bike rental options to both residents and tourists, aiming to encourage eco-friendly transportation.

BikeEase plans to leverage AI/ML capabilities to optimize operations, predict demand, and improve user experience. The goal is to build an intelligent analytics platform that helps understand rental patterns, seasonal trends, and operational efficiency.

The new platform will focus on:

1.   **Demand Forecasting Engine:** Predict rental demand based on historical data and external factors such as weather and seasons
2.   **Operational Optimization Engine:** Helps manage bike distribution and maintenance schedules
3.   **User Behavior Analysis:** Understand customer preferences and optimize marketing campaigns
4.   **Visualization Toolkit:** Provides insights through interactive dashboards for better decision-making

# **Project Statement**

Develop an end-to-end solution for data aggregation, cleaning, processing, and visualization using the provided bike rental dataset. The goal is to extract actionable insights to enhance decision-making capabilities.

Create a comprehensive data processing and visualization solution to analyze the bike rental dataset, identify trends, and provide valuable business insights to BikeEase.

# **Objective:**

To analyze the given bike rental dataset using Python and relevant libraries to perform data import, cleaning, processing, statistical analysis, and visualization

# **Input dataset:**
https://drive.google.com/file/d/1BAJ8iDpCJdfZSg1QS62RlMiJSs0O8MrG/view
"""

# Import the db file to Colab
from google.colab import files

# Upload the SQLite file
uploaded = files.upload()

"""# **Data Description**

The dataset consists of various features that impact bike rentals, such as weather conditions, seasonality, and operational factors. Below is a detailed description of the dataset:

```
**Date**: The date when the data was recorded
**Rented Bike Count**: The number of bikes rented during the given hour
**Hour**: The hour of the day (0-23)
**Temperature(°C)**: The recorded temperature in Celsius
**Humidity(%)**: The relative humidity percentage
**Wind speed (m/s)**: Wind speed measured in meters per second
**Visibility (10m)**: Visibility recorded in units of 10 meters
**Dew point temperature(°C):** The dew point temperature in Celsius
**Solar Radiation (MJ/m2)**: The amount of solar radiation received
**Rainfall(mm)**: The recorded rainfall in millimeters
**Snowfall (cm)**: The recorded snowfall in centimeters
**Seasons**: The season when the data was collected (e.g., Winter, Spring, Summer, Fall)
**Holiday**: Whether the day was a holiday or not
**Functioning Day**: Indicates whether the bike rental service was operational on that day
```

# **Steps to Perform**

# **Task 1: Import and Clean Data**
"""

# 1)
# Import relevant Python libraries for data manipulation and numerical operations:
# pandas, numpy, matplotlib, seaborn
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns

# 2)
# Load the dataset into a Pandas DataFrame from a CSV file.
# Filename: FloridaBikeRentals.csv

# Detecting the file enconding
import chardet
# Detect file encoding
with open('FloridaBikeRentals.csv', 'rb') as f:
    result = chardet.detect(f.read())
print("Detected encoding:", result['encoding'])
# --- answer adding the next code

# By enconding
df = pd.read_csv('FloridaBikeRentals.csv', encoding='ISO-8859-1')
df

# 3) Inspect the data:
# View the first few rows
df.head()

# 3.1) Inspect the data:
# shape (rows, columns)
df.shape

# 3.2) Inspect the data:
# column names
df.columns

# 3.3) Inspect the data:
# data types
df.dtypes

# 3.4) Inspect the data:
# Identify missing values and inconsistencies df.isna().sum() / df.isnull().sum()
df.isna().sum()

# 4) Handle missing values and data inconsistencies:
# Report missing values and suggest appropriate handling techniques (e.g., fill with mean, drop rows, etc.)
# No missing values detected.

# Check for duplicate records and remove them if necessary
df.duplicated().sum() # No duplicates detected.

# 5) Comment on data types and suggest optimizations for memory efficiency.
# Focus on columns such as Temperature, Humidity(%), Wind speed (m/s)
# df.info()
# df['Temperature(°C)'].info() #float64
# df['Humidity(%)'].info() #int64
# df['Wind speed (m/s)'].info() #float64


df1 = df.copy(deep=True)
df1

df1['Temperature(°C)']= df1['Temperature(°C)'].astype('float16')
df1['Humidity(%)'] = df1['Humidity(%)'].astype('int16')
df1['Wind speed (m/s)'] = df1['Wind speed (m/s)'].astype('float16')
df1.info()

# 6) Export the cleaned data to JSON format as bike_rental_cleaned.json
df.to_json('bike_rental_cleaned.json', orient='records') # without changes types
# df1.to_json('bike_rental_cleaned_changed.json', orient='records') # with changes types

df.info()

"""# 7) Write a short report summarizing observations about the data
- No missing values detect
- No Duplicated values detect

- Date, should be converted to datetime, now is object
- Temperature original float64 converted to float16
- Humidity(%) original int64 converted to int16
- Wind speed (m/s) original float64 converted to float16

- Before changes - memory usage: 958.3+ KB
- After changes - memory usage: 804.3+ KB

# **Task 2: Data Processing and Statistical Analysis**
"""

# 1) Perform transformations:
# Multiply Temperature by 10 for standardization
df['Temperature(°C)'] = df['Temperature(°C)'] * 10
df

# 1.1) Perform transformations:
# Scale Visibility to a range between 0 and 1 using MinMax scaling
from sklearn.preprocessing import MinMaxScaler

scaler = MinMaxScaler()
df['Visibility (10m)'] = scaler.fit_transform(df[['Visibility (10m)']])
df.head(5)

# 2) Conduct basic statistical analysis:
# Use describe() function for key columns like Temperature, Humidity(%), Rented Bike Count
df[['Temperature(°C)', 'Humidity(%)', 'Rented Bike Count']].describe()
# df[['Hour', 'Wind speed (m/s)', 'Visibility (10m)', 'Dew point temperature(°C)', 'Solar Radiation (MJ/m2)', 'Rainfall(mm)', 'Snowfall (cm)']].describe()

# 2.1) Conduct basic statistical analysis:
# Compare the results with raw dataset statistic
# df.describe()
df[['Seasons', 'Holiday', 'Functioning Day']].describe()
df['Date'].describe()

# 3) Identify columns that are not suitable for statistical analysis and recommend possible datatype changes
# Date #objet -> we can change to datetime
# Seasons	- object - The season when the data was collected (e.g., Winter, Spring, Summer, Fall) -> we can change to categorical
# Holiday	- object - Whether the day was a holiday or not -> we can change to categorical
# Functioning Day	- object - Indicates whether the bike rental service was operational on that day -> we can change to categorical

# Suggestion
# Feature Eng. # create new colum date+hour, and all information together

# 4) Export the processed data to a CSV file named bike_rental_processed.csv
df.to_csv('bike_rental_processed.csv', index=False)

"""# **5) Prepare a short report on statistical observations and insights**
- We did multiply Temperature by 10 for standardization
- We did Scale Visibility to a range between 0 and 1 using MinMax scaling

- Date #objet -> we can change to datetime
- Seasons	- object - The season when the data was collected (e.g., Winter, Spring, Summer, Fall) -> we can change to categorical
- Holiday	- object - Whether the day was a holiday or not -> we can change to categorical
- Functioning Day	- object - Indicates whether the bike rental service was operational on that day -> we can change to categorical

- Suggestion
- - Feature Eng. # create new colum date+hour, and all information together

# **Task 3: Data Analysis with Pandas**
"""

# 1) Identify categorical and numerical variables
# Focus on columns such as Seasons, Holiday, and Functioning Day
df2_encoded = pd.get_dummies(df, columns=['Seasons', 'Holiday', 'Functioning Day'], prefix='encode')
df2_encoded

# 2) Perform pivoting operations on the dataset based on categorical columns:
# Group by Seasons and calculate the average rented bike count
df_pivot1 = pd.pivot_table(df, index='Seasons', values='Rented Bike Count', aggfunc='mean')
df_pivot1

# 2.1) Perform pivoting operations on the dataset based on categorical columns:
# Analyze trends across Holiday and Functioning Day
df_pivot2 = pd.pivot_table(df, index=['Holiday', 'Functioning Day'], values='Rented Bike Count', aggfunc='sum')
df_pivot2

# 3) Create distribution tables:
# Temperature and Rented Bike Count distribution by Hour
# df_pivot3 = pd.pivot_table(df, index='Hour', values=['Temperature(°C)', 'Rented Bike Count'], aggfunc='count')
# df_pivot3

df_pivot3 = pd.pivot_table(df, values=['Temperature(°C)', 'Rented Bike Count'], index='Hour', aggfunc=['count','mean', 'std', 'min', 'max'])
df_pivot3

# df_pivot3 = pd.pivot_table(df, index='Hour', values=['Temperature(°C)', 'Rented Bike Count'], aggfunc='count')
# df_pivot3 = df_pivot3.reset_index()  # Reset the index separately
# df_pivot3

# df_distribuition = df.groupby('Hour')[['Temperature(°C)', 'Rented Bike Count']]
# print(df_distribuition)

# dist_table = df['Hour'].reset_index()
# dist_table.columns = ['Temperature(°C)', 'Rented Bike Count']
# print("Distribution Table:")
# dist_table

# 3.1) Create distribution tables:
# Seasons and Rented Bike Count distribution
df1_pivot4 = pd.pivot_table(df, index=['Seasons'], values='Rented Bike Count', aggfunc=['count','mean', 'std', 'min', 'max'])
df1_pivot4

# 4) Encode categorical variables and save data as "Rental_Bike_Data_Dummy.csv"
# df2_encoded = pd.get_dummies(df, columns=['Seasons', 'Holiday', 'Functioning Day'], prefix='encode')
# df2_encoded # done
df2_encoded.to_csv('Rental_Bike_Data_Dummy.csv', index=False)

"""# **Task 4: Data Visualization**"""

# 1) Import visualization libraries (matplotlib, seaborn)
import matplotlib.pyplot as plt
import seaborn as sns

# 2) Select appropriate visualization techniques for the data:
# Bar plot for average rentals by Seasons
bar_plot = df_pivot1.plot(kind='bar', figsize=(8, 6))
bar_plot.set_xlabel('Seasons')
bar_plot.set_ylabel('Average Rentals')
bar_plot.set_title('Average Rentals by Seasons')
plt.xticks(rotation=45)

filename = '/content/average_rentals_by_seasons.png'
plt.savefig(filename, dpi=300, bbox_inches='tight')

# Show the plot
plt.show()

# Download the saved file
files.download(filename)

# 2.1) Select appropriate visualization techniques for the data:
# Line plot showing hourly rentals throughout the day

sns.lineplot(data=df, x ='Hour', y='Rented Bike Count', hue="Seasons")
plt.title('Count of Rentals Throughout the Hour/Seasons')
plt.xlabel('Hour of the Day')
plt.ylabel('Number of Rentals')
filename = '/content/count_rentals_by_hourseason.png'
plt.savefig(filename, dpi=300, bbox_inches='tight')

# Show the plot
plt.show()

# Download the saved file
files.download(filename)

# 2.2) Select appropriate visualization techniques for the data:
# Heatmap showing correlation among numerical variables
numeric_df = df[['Temperature(°C)', 'Humidity(%)', 'Wind speed (m/s)', 'Visibility (10m)', 'Dew point temperature(°C)', 'Solar Radiation (MJ/m2)', 'Rainfall(mm)', 'Snowfall (cm)']]
correlation_matrix = numeric_df.corr()

plt.figure(figsize=(10, 8))
sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', fmt=".2f")
plt.title('Correlation Heatmap')
plt.xticks(rotation=45)

filename = '/content/correlation_heatmap.png'
plt.savefig(filename, dpi=300, bbox_inches='tight')

# Show the plot
plt.show()

# Download the saved file
files.download(filename)

# 2.3) Select appropriate visualization techniques for the data:
# Box plot to identify outliers in Temperature and Rented Bike Count

# Define temperature bins
bins = [0, 10, 20, 30, 40]  # Adjust based on your data range
labels = ["Cold (0-10°C)", "Mild (11-20°C)", "Warm (21-30°C)", "Hot (31+°C)"]

df1["temp_category"] = pd.cut(df["Temperature(°C)"], bins=bins, labels=labels)

sns.boxplot(x="temp_category", y="Rented Bike Count", data=df1)
plt.title("Bike Rentals by Temperature Category")
plt.xlabel('Temperature C - Categories')
plt.ylabel('Rented Bike Count')
plt.xticks(rotation=45)

filename = '/content/bike_rentals_by_temperature_category.png'
plt.savefig(filename, dpi=300, bbox_inches='tight')

# Show the plot
plt.show()

# Download the saved file
files.download(filename)

"""# **3) Record observations and insights from visualizations**

- In the summer more quantity of rental, then in autumm, more then spring,
- Winter low seasons for rental.
- In the morning low rentals quantity,
- In the evening high rentals quantity,
- And also at around nine in the morning there is a significancy rentals
- No functional days only in summer, autumn, and spring.
- On the heatmap: the data shows, more numbers of negatively correlated values and some are positively correlated but a few in numbers.
- In the cold range temperature, the average rented bike is low, compared to hot temperature days is high.



"""

# 4) Save plots and observations for reporting purposes
# Yes